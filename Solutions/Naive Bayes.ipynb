{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes solution in Python 3.6\n",
    "##### Ruihang Du\n",
    "The following is a Python 3 implementation of Naive Bayes spam detection algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Refresher on Naive Bayes:\n",
       "\\begin{align}\n",
       "P(y_i \\mid w_1, w_2, \\dots, w_n) &\\propto P(w_1, w_2, \\dots, w_n \\mid y_i)P(y_i)\\\\\n",
       "&= P(w_1 \\mid y_i)P(w_2 \\mid y_i)\\dots P(w_n \\mid y_i)P(y_i) &\\text{Under the independence assunption}\n",
       "\\end{align}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "Refresher on Naive Bayes:\n",
    "\\begin{align}\n",
    "P(y_i \\mid w_1, w_2, \\dots, w_n) &\\propto P(w_1, w_2, \\dots, w_n \\mid y_i)P(y_i)\\\\\n",
    "&= P(w_1 \\mid y_i)P(w_2 \\mid y_i)\\dots P(w_n \\mid y_i)P(y_i) &\\text{Under the independence assunption}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you are using Python 2, make sure you do the following:\n",
    "```Python\n",
    "from __future__ import division, print_function\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam_files\\spam_train.csv', encoding = \"ISO-8859-1\")\n",
    "data = df[['label', 'text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the __read_csv__ function in __Pandas__ to create a data frame from a csv file (Pandas is not required but generally makes your life easier dealing with formatted data).\n",
    "\n",
    "Remember to change the path to the data file accordingly (use slash instead of backslash if you are using Linux).\n",
    "\n",
    "Then we select only the __\"label\"__ column and the __\"text\"__ column from the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_dict = {}\n",
    "spam_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we initialize the dictionarys to store the __number of occurrences__ of each word in __spam__ and __ham__ emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_df = data.loc[(data.label == 'spam')].reset_index(drop=True)\n",
    "ham_df = data.loc[(data.label == 'ham')].reset_index(drop=True)\n",
    "            \n",
    "for i in range(len(spam_df)):\n",
    "    # look at every word in the email\n",
    "    for w in spam_df['text'][i].split(' '):\n",
    "        if w in spam_dict:\n",
    "            spam_dict[w] += 1       # update the count of this word\n",
    "        else:\n",
    "            spam_dict[w] = 1        # record the first occurrence\n",
    "            \n",
    "for i in range(len(ham_df)):\n",
    "    for w in ham_df['text'][i].split(' '):\n",
    "        if w in ham_dict:\n",
    "            ham_dict[w] += 1\n",
    "        else:\n",
    "            ham_dict[w] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we begin to fill in each dictionary.\n",
    "\n",
    "The first two lines separate the spam data and ham data to two __different tables__. Then for __each table__ we enumerate all the emails and record the number of occurrences for each single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_spam, num_ham = sum(spam_dict.values()), sum(ham_dict.values())\n",
    "\n",
    "for key in spam_dict:\n",
    "    spam_dict[key] /= num_spam\n",
    "    \n",
    "for key in ham_dict:\n",
    "    ham_dict[key] /= num_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the count of words in each dictionary to __probabilities__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Remember that for each word $w_i$, we need to calculate $P(w_i \\mid spam)$ and $P(w_i \\mid ham)$, which translates to\n",
       "\"Given that the email is a spam (or ham), how likely is it that I see the word $w_i$?\"\n",
       "\\begin{align*}\n",
       "P(w_i \\mid spam) &= \\frac{P(w_i, spam)}{P(spam)}\\\\\n",
       "&= \\frac{\\# \\text{$w_i$ occurred in spam}}{\\# \\text{spam}}\n",
       "\\end{align*}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "Remember that for each word $w_i$, we need to calculate $P(w_i \\mid spam)$ and $P(w_i \\mid ham)$, which translates to\n",
    "\"Given that the email is a spam (or ham), how likely is it that I see the word $w_i$?\"\n",
    "\\begin{align*}\n",
    "P(w_i \\mid spam) &= \\frac{P(w_i, spam)}{P(spam)}\\\\\n",
    "&= \\frac{\\# \\text{$w_i$ occurred in spam}}{\\# \\text{spam}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9034907597535934\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('spam_files\\spam_test.csv', encoding = \"ISO-8859-1\")\n",
    "test_data = df[['label', 'text']]\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "# For each text\n",
    "for i in range(len(test_data)):\n",
    "    total += 1\n",
    "    txt_to_eval = test_data['text'][i]\n",
    "    # split up the text into words and lookup in the spam_dict\n",
    "    words = txt_to_eval.split(' ')\n",
    "    l_s, l_h = 1, 1    # initialize likelihoods\n",
    "    for w in words:    \n",
    "        # First evaluate the likelihood of it being a spam\n",
    "        if w in spam_dict:\n",
    "            l_s *= spam_dict[w]\n",
    "        else:\n",
    "            l_s *= 1/(num_spam + 1)\n",
    "    \n",
    "        # Then evaulate the likelihood of it being a ham\n",
    "        if w in ham_dict:\n",
    "            l_h *= ham_dict[w]\n",
    "        else:\n",
    "            l_h *= 1/(num_ham + 1)\n",
    "    \n",
    "    l_s *= num_spam/(num_spam + num_ham)\n",
    "    l_h *= num_ham/(num_spam + num_ham)\n",
    "    \n",
    "    if (l_s > l_h and test_data['label'][i] == 'spam') \\\n",
    "    or (l_s < l_h and test_data['label'][i] == 'ham'):\n",
    "        correct += 1\n",
    "\n",
    "print(\"accuracy: {}\".format(correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we then test our NB classifier on the test set.\n",
    "\n",
    "For each test email, we calculate the likelihood of it being a spam and that of it being a ham. We then predict the class based on whichever likelihood is greater."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
